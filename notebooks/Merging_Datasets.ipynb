{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3823f863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading raw datasets...\n",
      "   - Dropped ['pubDate', 'guid', 'link'] from BBC.\n",
      "2. Standardizing Labels...\n",
      "--> Total Fake News: 36509\n",
      "--> Target Real News: 36509\n",
      "    - From WELFake: 18254\n",
      "    - From BBC: 18255\n",
      "5. Truncating texts to max 600 chars...\n",
      "4. Saving combined dataset to ../data/news_dataset.csv...\n",
      "==============================\n",
      "SUCCESS! Final Dataset Stats:\n",
      "Total Rows: 73018\n",
      "Class Balance (0=Fake, 1=Real):\n",
      "labels\n",
      "0    36509\n",
      "1    36509\n",
      "Name: count, dtype: int64\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Load raw datasets, remove irrelevant columns, merge using a \n",
    "# balanced hybrid strategy (50% Fake / 50% Real split between WELFake and BBC),\n",
    "# and truncate text to mitigate length bias before saving.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the character limit per article to force the model to focus on content \n",
    "# rather than length (Fake articles tend to be longer than BBC ones).\n",
    "CHAR_LIMIT = 600\n",
    "OUTPUT_FILE = \"../data/news_dataset.csv\"\n",
    "\n",
    "# LOAD DATASETS\n",
    "print(\"1. Loading raw datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load WELFake dataset\n",
    "    df_large = pd.read_csv(\"../data/WELFake_Dataset.csv\")\n",
    "    \n",
    "    # Load BBC News dataset\n",
    "    df_bbc = pd.read_csv(\"../data/bbc_news.csv\")\n",
    "    \n",
    "    # Drop metadata columns not required for training\n",
    "    cols_to_drop = ['pubDate', 'guid', 'link']\n",
    "    df_bbc = df_bbc.drop(columns=cols_to_drop, errors='ignore')\n",
    "    print(f\"   - Dropped {cols_to_drop} from BBC.\")\n",
    "    \n",
    "    # Standardize column names: rename description to text\n",
    "    df_bbc = df_bbc.rename(columns={'description': 'text'})\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"CRITICAL ERROR: File not found. {e}\")\n",
    "    raise\n",
    "\n",
    "# STANDARDIZE LABELS\n",
    "print(\"2. Standardizing Labels...\")\n",
    "\n",
    "# Invert WELFake labels to match target schema: 0 = Fake, 1 = Real.\n",
    "# Original: 1=Fake, 0=Real -> New: 0=Fake, 1=Real.\n",
    "df_large['label'] = 1 - df_large['label'] \n",
    "\n",
    "# BBC articles to 1 (Real)\n",
    "df_bbc['label'] = 1\n",
    "\n",
    "# Filter to keep only essential columns and remove nulls\n",
    "df_large = df_large[['title', 'text', 'label']].dropna()\n",
    "df_bbc = df_bbc[['title', 'text', 'label']].dropna()\n",
    "\n",
    "# HYBRID SAMPLING STRATEGY\n",
    "# Step A: Isolate all Fake news (Label 0) to serve as the anchor\n",
    "df_fakes = df_large[df_large['label'] == 0]\n",
    "n_fakes = len(df_fakes)\n",
    "print(f\"--> Total Fake News: {n_fakes}\")\n",
    "\n",
    "# Step B: Determine the target count for Real news to ensure a balanced dataset\n",
    "target_total_real = n_fakes\n",
    "half_real = target_total_real // 2\n",
    "\n",
    "print(f\"--> Target Real News: {target_total_real}\")\n",
    "print(f\"    - From WELFake: {half_real}\")\n",
    "print(f\"    - From BBC: {target_total_real - half_real}\")\n",
    "\n",
    "# Step C: Sample 50% of the required Real news from WELFake\n",
    "df_real_welfake = df_large[df_large['label'] == 1].sample(n=half_real, random_state=42)\n",
    "\n",
    "# Step D: Sample the remaining Real news from BBC\n",
    "needed_bbc = target_total_real - half_real\n",
    "\n",
    "if len(df_bbc) >= needed_bbc:\n",
    "    df_real_bbc = df_bbc.sample(n=needed_bbc, random_state=42)\n",
    "else:\n",
    "    print(f\"WARNING: BBC only has {len(df_bbc)} articles. Taking all.\")\n",
    "    df_real_bbc = df_bbc\n",
    "\n",
    "# MERGE AND SHUFFLE\n",
    "# Combine the Fake, WELFake Real, and BBC Real dataframes\n",
    "df_final = pd.concat([df_fakes, df_real_welfake, df_real_bbc], axis=0)\n",
    "\n",
    "# Shuffle the dataset and reset the index\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Rename label to labels\n",
    "df_final = df_final.rename(columns={'label': 'labels'})\n",
    "\n",
    "# TRUNCATION\n",
    "print(f\"5. Truncating texts to max {CHAR_LIMIT} chars...\")\n",
    "# Ensure text is string format and slice to the character limit\n",
    "df_final['text'] = df_final['text'].astype(str).apply(lambda x: x[:CHAR_LIMIT])\n",
    "\n",
    "# SAVE TO DISK\n",
    "print(f\"4. Saving combined dataset to {OUTPUT_FILE}...\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"SUCCESS! Final Dataset Stats:\")\n",
    "print(f\"Total Rows: {len(df_final)}\")\n",
    "print(f\"Class Balance (0=Fake, 1=Real):\")\n",
    "print(df_final['labels'].value_counts())\n",
    "print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
